# -*- coding: utf-8 -*-
"""NaiveBayesTwitterSenitmentClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A32A_9JMolx1FJtKBtFujmGAvot8RV1e
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("jp797498e/twitter-entity-sentiment-analysis")

print("Path to dataset files:", path)

import pandas as pd

# Read the training file
df = pd.read_csv(path + '/twitter_training.csv', header=None, names=['id', 'entity', 'sentiment', 'tweet'])

# Keep only 'tweet' and 'sentiment' columns
df = df[['tweet', 'sentiment']]

# Map 'Positive' sentiment to 1 and 'Negative' sentiment to 0
df['sentiment'] = df['sentiment'].map({'Positive': 1, 'Negative': 0})

# Display the first few rows of the modified DataFrame
print(df.head())

#Convert df series to numpy array, typecast sentiment column to int
import numpy as np
df['tweet'] = df['tweet'].to_numpy()
df['sentiment'] = df['sentiment'].to_numpy()
df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['tweet'])
df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['sentiment'])
df['sentiment'] = df['sentiment'].astype(int)

from sklearn.model_selection import train_test_split

# Assume df is your DataFrame and y is your target (or None if unsupervised)
# 1) First split off the test set (20% of the data).
train_val, test = train_test_split(
    df,
    test_size=0.2,        # 20% for test
    random_state=42,      # for reproducibility
    shuffle=True          # shuffle before splitting
)

# 2) Then split the remaining 80% into train (60%) and validation (20%)
#    Since train_val is 80%, to get 20% of original for val, we need val_size = 0.25 of train_val.
train, val = train_test_split(
    train_val,
    test_size=0.25,       # 0.25 * 0.8 = 0.2 of original
    random_state=42,
    shuffle=True
)
# 3) Reset indices so they run from 0
train = train.reset_index(drop=True)
val   = val.reset_index(drop=True)
test  = test.reset_index(drop=True)
# Check proportions
print(f"Train: {len(train)/len(df):.2%}")
print(f"Val:   {len(val)/len(df):.2%}")
print(f"Test:  {len(test)/len(df):.2%}")

#Preprocess tweets ->
#1. Lowercase letters
#2. Remove URLS, hastags, punctuations
#3. Tokenize the sentences
#4. Remove stop words
#5. Stemming
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize, sent_tokenize

# Download required resources (only once)
nltk.download('punkt_tab')
nltk.download('stopwords')

def preprocess_tweet(text):
    # 1. Lowercase
    text = text.lower()

    # 2. Remove URLs, mentions, hashtags, and punctuation
    text = re.sub(r"http\S+|@\S+|#\S+", "", text)
    text = re.sub(r"[^\w\s]", "", text)

    # 3. Sentence tokenization
    sentences = sent_tokenize(text)

    # Initialize stop words and stemmer
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()

    # 4. Word tokenization, stop word removal, and stemming
    processed_sentences = []
    for sentence in sentences:
        words = word_tokenize(sentence)
        filtered = [stemmer.stem(word) for word in words if word not in stop_words]
        processed_sentences.append(filtered)

    return processed_sentences

preprocessed_tweets_train = []
for tweet in train['tweet']:
    preprocessed_tweets_train.append(preprocess_tweet(tweet))

preprocessed_tweets_train[0]
# 1. Build mask: True for tweets that are non-empty
mask = [bool(tweet) for tweet in preprocessed_tweets_train]

# 2. Filter your list of preprocessed tweets
preprocessed_tweets_train = [
    tweet for tweet, keep in zip(preprocessed_tweets_train, mask)
    if keep
]

# 3. Drop the same rows from your DataFrame and reset index
train = train[mask].reset_index(drop=True)

uniquewords = []
for tweet in preprocessed_tweets_train:
    for sentence in tweet:
        for word in sentence:
            if word not in uniquewords:
                uniquewords.append(word)

print(len(preprocessed_tweets_train))

from collections import Counter

# initialize your sentimentâ€specific counters
word_freq_pos = Counter()
word_freq_neg = Counter()

for tokens_per_sentence, sentiment in zip(preprocessed_tweets_train, train['sentiment']):
    # flatten all sentences in the tweet into one list of tokens
    tweet_words = set(word
                      for sentence in tokens_per_sentence
                      for word in sentence)
    # increment the appropriate counter for each unique word
    if sentiment == 0:
        word_freq_neg.update(tweet_words)
    else:
        word_freq_pos.update(tweet_words)

pos_class = 0
neg_class = 0
for word,freq in word_freq_pos.items():
    pos_class += freq
for word,freq in word_freq_neg.items():
    neg_class += freq

V = len(uniquewords)
print(V)

import math
y_train = []
neg_count = (train['sentiment'] == 0).sum()
pos_count = (train['sentiment'] == 1).sum()
print(f"Negative: {neg_count}")
print(f"Positive: {pos_count}")
log_prior = math.log(pos_count/neg_count)
for tweet in preprocessed_tweets_train:
  if not tweet:
    continue
  for words in tweet[0]:
    pos_prob = (word_freq_pos[words] + 1)/(pos_class+V)
    neg_prob = (word_freq_neg[words]+1)/(neg_class+V)
    log_prior += math.log(pos_prob/neg_prob)
  if log_prior > 0:
    y_train.append(1)
  else:
    y_train.append(0)

match_calculated = 0
for i in range(0,len(train)):
  if y_train[i] == train['sentiment'][i]:
    match_calculated += 1
print(f"Training Accuracy: {match_calculated/len(train)}")

cal_lambda_word = []
for word in uniquewords:
  pos_prob = (word_freq_pos[word] + 1)/(pos_class+V)
  neg_prob = (word_freq_neg[word]+1)/(neg_class+V)
  cal_lambda_word.append(math.log(pos_prob/neg_prob))

val_preprocessed_tweets = []
for tweet in val['tweet']:
    val_preprocessed_tweets.append(preprocess_tweet(tweet))

mask = [bool(tweet) for tweet in val_preprocessed_tweets]

# 2. Filter your list of preprocessed tweets
val_preprocessed_tweets = [
    tweet for tweet, keep in zip(val_preprocessed_tweets, mask)
    if keep
]

# 3. Drop the same rows from your DataFrame and reset index
val = val[mask].reset_index(drop=True)

len(val_preprocessed_tweets)

log_prior = math.log(pos_count/neg_count)
y_val = []
for val_tweet in val_preprocessed_tweets:
  for word in val_tweet[0]:
    if word in uniquewords:
      log_prior += cal_lambda_word[uniquewords.index(word)]
  if log_prior > 0:
    y_val.append(1)
  else:
    y_val.append(0)

match_calculated = 0
for i in range(0,len(val)):
  if y_val[i] == val['sentiment'][i]:
    match_calculated += 1
print(f"Validation Accuracy: {match_calculated/len(val)}")

test_preprocessed_tweets = []
for tweet in test['tweet']:
    test_preprocessed_tweets.append(preprocess_tweet(tweet))

mask = [bool(tweet) for tweet in test_preprocessed_tweets]

# 2. Filter your list of preprocessed tweets
test_preprocessed_tweets = [
    tweet for tweet, keep in zip(test_preprocessed_tweets, mask)
    if keep
]

# 3. Drop the same rows from your DataFrame and reset index
test = test[mask].reset_index(drop=True)

log_prior = math.log(pos_count/neg_count)
y_test = []
for val_tweet in test_preprocessed_tweets:
  for word in val_tweet[0]:
    if word in uniquewords:
      log_prior += cal_lambda_word[uniquewords.index(word)]
  if log_prior > 0:
    y_test.append(1)
  else:
    y_test.append(0)

match_calculated = 0
for i in range(0,len(test)):
  if y_test[i] == test['sentiment'][i]:
    match_calculated += 1
print(f"Test Accuracy: {match_calculated/len(test)}")

